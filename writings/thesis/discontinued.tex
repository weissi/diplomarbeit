\JWlone{Discontinued Approaches}
\label{sec:discontinued}

This chapter briefly describes the approaches that have been investigated (and
partly implemented) but were considered not good enough to reach the goal. The
approaches are ordered chronologically by date of examination to document the
evolution of the methodology.


%
% LBS
%
\JWltwo{Linear Bijection Straight--Line Programs}
\label{sec:using-lbs}

The first approach pursued in this thesis to transform general formulas to
affine functions suitable for OAFEs was via Linear Bijection Straight--Line
Programs.  The results seemed promising at the first glance but a problem
emerged when using multiplications. The problem already becomes apparent as
Cleve writes in the abstract of his paper \cite{cleve91}:

\begin{quote}
  We show that, over an arbitrary ring, for any fixed $\epsilon > 0$, all
  balanced algebraic formulas of size $s$ are computed by algebraic
  straight--line programs that employ a constant number of registers and have
  length $O(s^{1 + \epsilon})$.
\end{quote}

\noindent{}Two problems arise: Cleve's approach does not support \emph{Square
and Multiply} and it is slightly polynomial. However, the approach
is partly implemented as explained in detail in the next few sections but
neither the code nor the ideas are used for the final version of this thesis.

\JWlthree{From Arithmetic Formulas to Matrix Multiplications}
\label{sec:FormulasToMatrixMuls}

The definition of formulas is the same as Cleve's \cite{cleve91}: Formulas are
circuits that are trees. A postorder traversal is enough to evaluate the
formula. This section describes the evaluation of such a formula using
\emph{linear bijection straight--line programs} (LBS programs) \cite{cleve91}
which use at most $\omega$ registers. An LBS program can be simulated by matrix
multiplications, one statement is simulated by one matrix multiplication. The
matrices are elements of $SL_w(K)$, the special linear group consisting of
$\omega \times \omega$ matrices with determinant $1$ (and $K$ a field).

A LBS program consists of assignment statements of the following
forms where $R_{1,\ldots,\omega}$ denote registers, $c \in K$ constants and $x_u
\in K$ the formula's inputs:

\begin{align*}
R_j & \leftarrow R_j + (R_i \cdot c) \\
R_j & \leftarrow R_j - (R_i \cdot c) \\
R_j & \leftarrow R_j + (R_i \cdot x_u) \\
R_j & \leftarrow R_j - (R_i \cdot x_u)
\end{align*}


\JWlfour{Transformation of Formulas to LBS Programs}

The goal is to transform a register $R_{out}$ with a initial value of $0$ to be
transformed like $R_{out} \leftarrow R_{out} + R_{one} \cdot f(x_G,x_D)$. The
special register $R_{one}$ holds a constant $1$. This can be achieved by
induction as follows.  For the exact definitions, proofs and algorithms how to
transform arbitrary formulas to LBS programs, see \cite{cleve91}.


\JWlfive{Depth $d = 0$}

The construction of the LBS for $d = 0$ is straightforward:
$R_j \leftarrow R_j \pm R_i \cdot c$ or $R_j \leftarrow R_j \pm R_i \cdot x_u$ .


\JWlfive{Depth $d > 0$}

Two LBS programs that have the effect of $R_j \leftarrow R_j \pm R_i \cdot
l(x_G, x_D)$  and $R_j \leftarrow R_j \pm R_i \cdot r(x_G, x_D)$ express
formulas of depth $d > 0$ using only formulas of depth $d - 1$. Repeated
until $d = 0$, arbitrary formulas can be written as LBS programs. The following
two sections show the induction step for additions and multiplications.

\JWlsix{Additive:} The construction of an LBS program doing $R_j \leftarrow R_j
+ R_i \cdot (l + r)(x_G, x_D)$ can be achieved by the following LBS program.

\begin{align*}
R_j & \leftarrow R_j + R_i \cdot l(x_G, x_D) \\
R_j & \leftarrow R_j + R_i \cdot r(x_G, x_D)
\end{align*}

Alike for $R_j \leftarrow R_j - R_i \cdot (l + r)(x_G, x_D)$

\begin{align*}
R_j & \leftarrow R_j - R_i \cdot l(x_G, x_D) \\
R_j & \leftarrow R_j - R_i \cdot r(x_G, x_D)
\end{align*}


\JWlsix{Multiplicative:} The construction of an LBS program doing $R_j
\leftarrow R_j + R_i \cdot (l \cdot r)(x_G, x_D)$ is less obviously achieved by
the following LBS program.

\begin{align*}
R_k & \leftarrow R_k - R_j \cdot r(x_G, x_D) \\
R_j & \leftarrow R_j + R_i \cdot l(x_G, x_D) \\
R_k & \leftarrow R_k + R_j \cdot r(x_G, x_D) \\
R_j & \leftarrow R_j - R_i \cdot l(x_G, x_D)
\end{align*}

Alike for $R_j \leftarrow R_j - R_i \cdot (l \cdot r)(x_G, x_D)$

\begin{align*}
R_k & \leftarrow R_k - R_j \cdot r(x_G, x_D) \\
R_j & \leftarrow R_j - R_i \cdot l(x_G, x_D) \\
R_k & \leftarrow R_k + R_j \cdot r(x_G, x_D) \\
R_j & \leftarrow R_j + R_i \cdot l(x_G, x_D)
\end{align*}


\JWlfour{Implementation State}

The current implementation is a Haskell module which features the transformation
from an arithmetic expression to an LBS program. The LBS program then transforms
easily to the matrices. The multiplication of the matrices yield the result. The
implementation can be found in the file \JWpath{lib/Codec/LBS.hs} of the code
tree of this thesis (see Section \ref{sec:src-org} and
\ref{sec:code-availability}). Exemplary, the LBS program to evaluate the
function $f(x_G,x_D) = 3x \cdot (x_G + x_D^2)$ which will hold the result in
\texttt{R1}:

\begin{lstlisting}
R1 <- R1 - R2 * Xg
R1 <- R1 - R3 * Xd
R3 <- R3 - R2 * Xd
R1 <- R1 + R3 * Xd
R3 <- R3 + R2 * Xd
R2 <- R2 - R3 * Xg
R3 <- R3 + R0 * 3
R2 <- R2 + R3 * Xg
R3 <- R3 - R0 * 3
R1 <- R1 + R2 * Xg
R1 <- R1 - R3 * Xd
R3 <- R3 + R2 * Xd
R1 <- R1 + R3 * Xd
R3 <- R3 - R2 * Xd
R2 <- R2 - R3 * Xg
R3 <- R3 - R0 * 3
R2 <- R2 + R3 * Xg
R3 <- R3 + R0 * 3
\end{lstlisting}

\noindent{}The construction of the matrices is straightforward: The statement
$R_i \leftarrow R_i + (R_j \cdot \alpha)$ is equivalent to the $K^{\omega \times
\omega}$ identity matrix whose entry $i,j$ is set to $\alpha$.


\JWlthree{Grouping the Matrices}
\label{sec:matrix-grouping}

The grouping process is straightforward: From the process described in Section
\ref{sec:FormulasToMatrixMuls} matrices $\widehat{M_1}$ to $\widehat{M_n}$,
which each have the effect of exactly one LBS program statement, are obtained.
Using associativity, groups of a variable amount of these matrices can be built.
Each group is complete when there is at least one reference to the \emph{other
party's input} $x_D$. The matrices $M_1$ to $M_m$ are the result of this step.
The following properties hold:

\begin{align*}
n & \geq m \\
\prod_{i=1}^m M_i & = \prod_{j=1}^n \widehat{M_j}
\end{align*}

\JWlthree{Garbling the Matrices}
\label{sec:matrix-garbling}

Let $D_L$ be the $\omega \times \omega$ matrix whose entry $2,2$ is $1$, and
whose other entries are $0$. Multiplication of $D_L$ selects the second row of
matrices multiplied on the right of $D_L$. Let $D_R$ be the $\omega \times
\omega$ matrix whose entry $1,1$ is $1$, and whose other entries are $0$. This
matrix will select the first column when multiplied on the left of any matrix.
Using additional matrices $S_1$ to $S_{m}$ uniformly at random and invertible,
$m$ garbled matrix groups can be built:

\begin{align*}
U_1 & = D_L M_1 S_1 \\
U_i & = S_{i-1}^{-1} M_i S_i &
\text{for $i \in \{n \in \mathbb{N} \big| 1 < n < m\}$}\\
U_m & = S_{m-1}^{-1} M_m D_R
\end{align*}

\noindent{} Hence, each $U_{1..m}$ does not reveal usable information by itself
\cite{cramer03}, but $\prod_{i=1}^m U_i$ does still calculate the desired
result.


\JWlthree{Evaluating the Matrices using OAFEs}

From Section \ref{sec:matrix-garbling} the matrices $U_{1..m} \in K^{\omega
\times \omega}$ are obtained. The matrices $U_{1..m}$ can be reshaped to vectors
$u_{1..m} \in K^{\omega^2}$. The vectors $u_{1..m}$ can be concatenated to one
large vector $\mu \in K^{m\omega^2}$. Next, two vectors $a$ and $b$ are deduced
that hold the following property ($a, b \in K^{m\omega^2}$, $x_D$ a scalar
variable, as in Section \ref{sec:matrix-grouping} David's input):

\begin{align}
a \cdot x_D + b = \mu
\end{align}

Using the $\prod^{\text{semi-int}}_{\text{OAFE}}$ protocol \cite{davidgoliath}
the function can now be evaluated securely.

\begin{itemize}

\item The setup is: $a, b$ as above, $\mathbb{F}_q = K$, $k = m\omega^2$ and $m$
known to both, David and Goliath

\item After applying the protocol, David is now able to evaluate the linear
functions to his result vector $y \in K^{m\omega^2} = GWh + \tilde{a}x_D +
\tilde{b}$ ($G$, $W$, $h$, $\tilde{a}$ ,$\tilde{b}$ as in the paper
\cite{davidgoliath})

\item The last but one step is to reshape $y$ to the matrices $F_{1..m}
\in K^{m\omega^2}$

\item Finally, the entry $2, 1$ of $\prod_{i=1}^m F_i$ is the desired result of
$f(x_G,x_D)$.

\end{itemize}


%
% DARE
%
\JWltwo{Decomposable Affine Randomized Encodings}
\label{sec:dare}

The second approach that was examined during this thesis, was to transform
general functions to affine functions using a slightly modified form of the
\emph{Decomposable Affine Randomized Encodings} (DAREs) from \emph{Garbled
Arithmetic Circuits} by Applebaum et al.\ \cite{gac2012}. The change is
necessary because this thesis uses OAFEs to evaluate the DAREs and does
therefore not depend on the \emph{learning with errors} (LWE) problem.  This
leads to further changes: The \emph{affinization gadget} \cite{gac2012} is
modified and the \emph{key--shrinking gadget} \cite{gac2012} is not needed.
Although the final version of this thesis does not directly contain anything
from Applebaum's garbled arithmetic circuits, some ideas are closely related and
inspired by Applebaum's paper.


\JWlthree{Definitions}
\label{sec:affinization_definitions}

These definitions are not from the original paper but are closely related. A
\emph{Linear Randomized Expression} (LRE) is an element of the set
$\mathcal{F}_{AR}$ ($K$ a finite field), a \emph{Decomposable Affine Randomized
Encoding} (DARE) an element of the set $\mathcal{E}_{AR}$. One important
constraint has to hold for all LREs and therefore for all DAREs too: After fully
evaluating an LRE, i.e.\ replacing the variable by its actual value, the now
constant LRE should reveal no more information than the result of a full
decoding of the respective DARE. The safety is proved in \emph{How to Garble
Arithmetic Circuits} \cite{gac2012}.

\begin{align*}
  \mathcal{V} = & \{ x \mid x~\text{a variable over}~K \} \\
%
  \mathcal{F}_{AR} = & \{ s \cdot x + i \mid s, i \in K, x \in \mathcal{V} \}
  \cup \{ v \mid v \in K \} \\
%
  \mathcal{E}_{AR} = & \{ (M, A) \mid
    M \subseteq \mathcal{F}_{AR} \times \mathcal{F}_{AR},
    A \subseteq, \mathcal{F}_{AR};
    A, M~\text{finite multi--sets} \}
%
\end{align*}


\JWlthree{Encoding}
\label{sec:affinization_encoding}

\begin{itemize}

\item A function $f_1(x_1, x_2) = x_1 + x_2$ can be securely encoded by
$ENC_A(f_1, r)$; $r \in K$ being uniformly at random.

\item A function $f_2(x_1, x_2, x_3) = x_1 \cdot x_2 + x_3$ can be securely
encoded by $ENC_M(f_2, r_1, r_2, r_3, r_4)$; $r_1, r_2, r_3, r_4 \in
K$, uniformly at random.

\end{itemize}

\begin{align*}
ENC_A(f_1, r) = \Big( & \emptyset, (x_1 + r, 1 \cdot x_2 - r)\Big) \\
ENC_M(f_2,  r_1, r_2, r_3, r_4) = \Bigg( & \bigg\{
\begin{pmatrix}1 \cdot x_1 - r_1\\1 \cdot x_2 - r_2\end{pmatrix} \bigg\}\\
,& \bigg\{r_2 \cdot x_1 -r_1r_2+r_3 \\
&\ ,\ r_1 \cdot x_2 + r_4 \\
&\ ,\ 1 \cdot x_3-r_3-r_4\bigg\} \Bigg)
\end{align*}


\JWlthree{Decoding}
\label{sec:affinization_decoding}

Decoding a fully evaluated DARE $\mathcal{E}_{AR} = \{(M,A)\}$ as $r =
DEC(\mathcal{E}_{AR})$ is
straightforward:

\begin{align*}
M' &= \Bigg\{ m_1 \cdot m_2\ \Bigg|\ \begin{pmatrix}m_1\\m_2\end{pmatrix}
\in M \Bigg\} \\
r & = \sum_{a \in A} a + \sum_{m \in M'} m
\end{align*}


\JWlthree{Randomized Variables}
\label{sec:rv}

Whenever a circuit is not directly transformable to one single DARE, sub--DAREs
get replaced by \emph{Randomized Variables} (RV). RVs do not appear in
Applebaum's paper \cite{gac2012}. RVs are just (sub--)DAREs that
get transmitted to the second party (see Section \ref{sec:dare}) which
fully evaluates them and saves the result just like an ordinary input variable
(such as $x_D$). Following DAREs may then use the RVs as pseudo--inputs. But
since a DARE reveals as much information as its decoded form, the original DARE
cannot just be transmitted as the final overall DARE. That would reveal
intermediate information. Therefore, RVs get an additional garbling step: Say
the following property holds for a variable $v$ being the decoded value of
a DARE $e$:

\begin{align*}
  v = DEC(e \in \mathcal{E}_{AR})
\end{align*}

\noindent{}Then a modified DARE $\hat{e} \in \mathcal{E}_{AR}^+$ decoding to a
value $\hat{v}$

\begin{align*}
\hat{v} = \alpha \cdot (v + \beta) = DEC(\hat{e})
\end{align*}

\noindent{}with secret keys $\alpha$ and
$\beta$---known only by the first party---is transmitted.


\JWlthree{Relation to the Final Version}

Some of the ideas and entities presented in this section relate to the final
version of this thesis. \emph{Decomposable Affine Randomized Encodings} (DAREs)
are closely related to \emph{Dual Randomized Affine Encodings} (DRAEs, Section
\ref{def:DRAE}). The difference is that an evaluated DARE is equivalent to the
value of the sub--circuit it models, DRAEs in contrast are still
encrypted (twofold). Because the DAREs are equal to their plain evaluation,
the old version presented in this section needed the \emph{Randomized Variables}
(RVs) that are not present in the final version. However, the final version
also assigns evaluated sub--circuits to variables, but since DRAEs are encrypted
anyways, there is no need for an additional encryption.


\JWlthree{Implementation State}

The ideas of this section were implemented, but the implementation did
evolve to the final version described in Chapter \ref{sec:methods}. If
there is interest in the old implementation, it can be recovered from this
thesis' source code tree by using the \JWTgit{} version control system.

% vim: set spell spelllang=en_us fileencoding=utf8 :
